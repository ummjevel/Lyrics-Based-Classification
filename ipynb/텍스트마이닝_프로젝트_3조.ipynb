{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "21Wo9MbmErIb",
        "dOVGpExSBIaz",
        "oZCUCn41Bvsn",
        "_B-RnPJ_6Pa8",
        "fH8kx6R37rje",
        "HkG90sK17i7K",
        "hwfJXJAq8HTA",
        "fX6upYry8P7R",
        "NCIfnh218XB2",
        "SuH6Eyuy8d0L",
        "VSd4VlS58npc",
        "nDCZhzGi8vDa",
        "k3wGw8Rr81D8",
        "mEXaknbV8-98"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 개요\n",
        "\n",
        "멜론, 벅스, 지니의 시대별 히트곡 가사로 분류한다.\n",
        "\n",
        "## 3조\n",
        "* 김성모, 이성연, 이현중, 전민정\n",
        "\n",
        "## 진행 순서\n",
        "\n",
        "1. 크롤링\n",
        "2. 전처리\n",
        "\n",
        "## 타임테이블\n",
        "\n",
        "https://timetr.ee/s/i48rvoYnhUOfgLkrswFhKFijqdVi8vVA\n",
        "\n",
        "## 레이블\n",
        "* 희망\n",
        "* 돈\n",
        "* 사랑 - 연애, 이별\n",
        "* 효 - 어머니, 아버지 ...\n",
        "* 우정 - 친구\n",
        "* 남녀평등 - 걸크러쉬\n",
        "* 욕설\n",
        "* 자연\n",
        "* 후회\n",
        "* 추억\n",
        "* SF\n",
        "* 음식\n",
        "* 자신감\n",
        "* 즐거움\n"
      ],
      "metadata": {
        "id": "r5DxUnDuAOwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 크롤링"
      ],
      "metadata": {
        "id": "sQgwGwcaAkyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멜론"
      ],
      "metadata": {
        "id": "u_Xe5mRdA8V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "a6jjtQXKBDE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years, singers, songs, lyrics = [],[],[],[]\n",
        "start_year = 1964\n",
        "final_year = 2022\n",
        "for i in tqdm(range(start_year, final_year)):\n",
        "\n",
        "  #각 년도별 top 100 노래를 담은 url 추출\n",
        "  url = 'https://www.melon.com/chart/age/list.htm?idx=1&chartType=YE&chartGenre=KPOP&chartDate='+str(i)+'&moved=Y'\n",
        "\n",
        "  headers = {'Referer':'https://www.melon.com/song/detail.htm?songId=33239419',\n",
        "            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36'}\n",
        "\n",
        "  resp = requests.get(url, headers = headers)\n",
        "  soup = BeautifulSoup(resp.content, 'lxml')\n",
        "  id_tag = soup.select('div.wrap button')\n",
        "  data = []\n",
        "\n",
        "  for j in range(len(id_tag)):\n",
        "    try:\n",
        "      #각 노래의 고유번호 data 리스트에 저장해주기\n",
        "      id_tag[j]['data-song-no']\n",
        "      data.append(id_tag[j]['data-song-no'])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  for dat in data:\n",
        "    link = 'https://www.melon.com/song/detail.htm?songId=' + dat\n",
        "\n",
        "    headers = {'Referer':'https://www.melon.com/song/detail.htm?songId=' + dat,\n",
        "              'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36'}\n",
        "\n",
        "\n",
        "    resp = requests.get(link, headers = headers)\n",
        "    soup = BeautifulSoup(resp.content, 'lxml')\n",
        "    lyric_tag = soup.select('div.wrap_lyric')\n",
        "    song_tag = soup.select('div.song_name')\n",
        "    try:\n",
        "      singer_tag = soup.select('div.artist a')\n",
        "      singer_tag[1]\n",
        "    except:\n",
        "      singer_tag = soup.select('div.artist')\n",
        "    raw = lyric_tag[0].text\n",
        "    lyric = raw[:-6].strip()\n",
        "    song = song_tag[0].text.strip()[2:].strip()\n",
        "    singer = singer_tag[0].text.strip()\n",
        "    lyrics.append(lyric)\n",
        "    songs.append(song)\n",
        "    singers.append(singer)\n",
        "    years.append(i)\n"
      ],
      "metadata": {
        "id": "tKnwcd_aDHVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 로 정리하여 출력\n",
        "dict_song = {'years':years,\n",
        "             'singers':singers,\n",
        "             'songs':songs,\n",
        "             'lyrics':lyrics}\n",
        "\n",
        "df = pd.DataFrame(dict_song)\n",
        "df.to_csv('songs_ranking.csv', index=False)"
      ],
      "metadata": {
        "id": "6JzOx9HoEEI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 벅스\n",
        "\n",
        "- 연도별 링크 가져오기"
      ],
      "metadata": {
        "id": "ehFhgl9eA6Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "years_urls, years = [], []\n",
        "years_url = 'https://music.bugs.co.kr/years'\n",
        "\n",
        "resp = requests.get(years_url)\n",
        "soup = BeautifulSoup(resp.content, 'lxml')\n",
        "a_tags = soup.select('a.title.hyrend')\n",
        "\n",
        "for a_tag in a_tags:\n",
        "\n",
        "  if a_tag['title'].split('년')[1][0] == '대':\n",
        "    continue\n",
        "\n",
        "  years_urls.append(a_tag['href'])\n",
        "  years.append(a_tag['title'].split('년')[0])\n"
      ],
      "metadata": {
        "id": "-McPGfqEBDgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 각 링크 내 곡목록 가져오기"
      ],
      "metadata": {
        "id": "bb4LZEaMCLGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_url, title, artist, year = [], [], [], []\n",
        "\n",
        "for i in tqdm(range(len(years_urls))):\n",
        "  url = years_urls[i]\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.content, 'lxml')\n",
        "\n",
        "  lyrics_urls = soup.select('table.trackList td a.trackInfo')\n",
        "  titles = soup.select('table.trackList th p.title')\n",
        "  artists = soup.select('section.sectionPadding.contents table.list.trackList td.left p.artist a')\n",
        "\n",
        "  # 여러 가수가 불렀을 경우 첫번째 가수만 가져오도록 처리.\n",
        "  for artist_tag in artists:\n",
        "    if artist_tag['title'] == '아티스트 전체보기':\n",
        "      continue\n",
        "    artist.append(artist_tag)\n",
        "\n",
        "  # 금지곡인 경우 a 태그 말고 span 태그이기에 따로 처리.\n",
        "  for title_tag in titles:\n",
        "    \n",
        "    title_a_tags = title_tag.select('a')\n",
        "    title_span_tags = title_tag.select('span')\n",
        "\n",
        "    if len(title_a_tags) > 0:\n",
        "      title.append(title_a_tags[0]['title'])\n",
        "    else:\n",
        "      title.append(title_span_tags[1].text)\n",
        "\n",
        "\n",
        "  lyrics_url.extend(lyrics_urls)\n",
        "  year.extend([years[i]]*len(lyrics_urls))\n",
        "\n",
        "  # if len(lyrics_url) != len(title):\n",
        "  #   print(i, ':',  url)\n",
        "\n",
        "print(len(lyrics_url), len(title), len(artist), len(year))\n"
      ],
      "metadata": {
        "id": "j1czrDAsCOfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 각 링크에서 추출한 가사 링크 접속해서 가사 추출하기"
      ],
      "metadata": {
        "id": "MRHFt30zCRAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lyrics = []\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'\n",
        "    ,'Referer': 'https://music.bugs.co.kr/track/6065263?wl_ref=list_tr_08_mab'\n",
        "}\n",
        "\n",
        "cookie = {'cookie':\n",
        "          '_ga=GA1.3.1816123160.1667278014; _gid=GA1.3.1405268211.1667278014; _gac_UA-134713316-19=1.1667278014.CjwKCAjw5P2aBhAlEiwAAdY7dEGLFCONQRkEoA0kvJenxh8wG6mMv1kbFZ0E2aS9CghY6E4Whd4fORoC6yEQAvD_BwE; _gac_UA-97174709-1=1.1667278014.CjwKCAjw5P2aBhAlEiwAAdY7dEGLFCONQRkEoA0kvJenxh8wG6mMv1kbFZ0E2aS9CghY6E4Whd4fORoC6yEQAvD_BwE; cto_bundle=8l24E19TTzRyYSUyQmxYZUhmbEoyWTRZMSUyRkppS2hWVTVhWmpmV3YlMkJpZSUyRkwlMkYxY001OXprT1dRRUdYSHdiTDhsOTJjdHFOU1JuRnpqU21TczNkemwzcyUyQjVLVG1LeTdRREpkbUxibXFZczA2OTVoTDJUM2xDR20lMkZpWmE3emN5dXJDeXg2MlM0OUc1WCUyRlRIVyUyQkxLelUxbUJINGJlN1B6SHdaYmV1cjBpRVhNTGlPbWxYdE9OdWpQNVNicTRnQk1MUk5SV3V3bnU; SCOUTER=x5r8m0krm2sd5h; PCID=16672780261962967733626; RC_RESOLUTION=1440*900; RC_COLOR=30; ACEUCI2=1; searchLogSessionId=6C1290A2-8B91-9CC0-9130-7E31289B6F98; ConnectionInfo=4ef8dd71f9769907d352320408bd6356%2C1%3A58081741%3Adeepshadow25%40naver.com%3A%40%3A%40ZGVlcHNoYWRvdzI1QG5hdmVyLmNvbTo3NzExMjgyNDI6ZTM1YjQ0NDYyNzE5MDliZjE5YmYzYWZjMjA4Mzk4NzU%2Cupdated%3A1667347249%2Cip%3A1.215.118.68; UserID=deepshadow25%40naver.com; UserSRL=58081741; chk=\"t1=t1&BugsNo=DGCERg7AtId&BugsId=x967AK1BhMK-5imhuGwVjxbFM68HzNVpl&key=10790173&BugsName=WB-WtSXyvamkJKVEK3k5JsOluZU9RTcRqUZBWqhDMGdV&BugsMemTp=e&t2=t2\"; cp=BugsNo%3DDGCERg7AtId%26BugsId%3Dx967AK1BhMK-5imhuGwVjxbFM68HzNVpl%26key%3D10790173%26BugsEncodingName%3D%25EC%259D%25B4%25EC%2584%25B1%25EC%2597%25B0%26BugsName%3D%C0%CC%BC%BA%BF%AC%26vstat%3DY%26ssn%3D1QNx-DqAQBq%26gender%3DM%26loginIp%3D1.215.118.68%26BugsCI%3DzKgmvnLFE9ET5HmhpPr1RiuPZJzNVszFF6BZS95sRMd5HiE6omeLL%2Bl4sLIumuiSouLitOl2uLywy8bqyfQ5VA%3D%3D%26BugsMemTp%3De; AccessToken=qRK8rbKQaVBeiX5C9SHttxndJWofSW3eKu_RfI2ERoBsrBTEmR-rubx; OauC=YTo0OntzOjEyOiJhY2Nlc3NfdG9rZW4iO3M6NTU6InFSSzhyYktRYVZCZWlYNUM5U0h0dHhuZEpXb2ZTVzNlS3VfUmZJMkVSb0JzckJURW1SLXJ1YngiO3M6MTM6InJlZnJlc2hfdG9rZW4iO3M6NTU6IlNfR2V5RThOb0t3RFlweU5JLWd0M3FpdWVjZWYtWFV3ckxORlNHZ2FSVEttVEFtVGJibUs2d3giO3M6ODoic2V0X2RhdGUiO3M6MTM6IjE2NjczNDcyNDk0MzQiO3M6MTA6ImV4cGlyZXNfaW4iO2k6MjUzNDYxNTt9; last_login_log_date_cookie=1667347250942; _ACR0=75ce47f94a08acb60af230e5c61cedb107343795; _lh_time_=1667348090866; wcs_bt=s_595467f7a42:1667348362; _ACU103005=1667278026423717846.1667348362328.1.0.717846MDR9AVRYYYX10.0.0.0.....; _ACS103005=534; JSESSIONID=CBC72D9EC555C97BF75B8114AA8A9E61.s830'}\n",
        "\n",
        "for i in tqdm(range(len(lyrics_url))): #\n",
        "  url = lyrics_url[i]['href']\n",
        "  resp = requests.get(url, headers=headers, cookies=cookie)\n",
        "  soup = BeautifulSoup(resp.content, 'lxml')\n",
        "\n",
        "  lyrics_tag = soup.select('div.lyricsContainer xmp')\n",
        "\n",
        "  if len(lyrics_tag) > 0:\n",
        "    lyrics.append(str(lyrics_tag[0]).replace('<xmp>', '').replace('</xmp>', '').replace('\\r\\n', ' '))\n",
        "  else:\n",
        "    lyrics.append('NULL')\n",
        "    print(i, ':', url, 'is NULL')\n",
        "\n"
      ],
      "metadata": {
        "id": "XBR7PComCUpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터 정제 후 csv 출력"
      ],
      "metadata": {
        "id": "9HtB1I6yCWZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "dict_lyrics = {\n",
        "    'lyrics_url': lyrics_url, 'title': title, 'artist': artist, 'year': year, 'lyrics': lyrics\n",
        "}\n",
        "\n",
        "df_lyrics = pd.DataFrame(dict_lyrics)\n",
        "df_lyrics.to_csv('df_lyrics.csv')\n",
        "\n",
        "df_lyrics2 = df_lyrics \n",
        "for i in range(len(df_lyrics2['artist'])):\n",
        "  # df_lyrics2['artist'][i] = df_lyrics2['artist'][i].text\n",
        "  df_lyrics2['lyrics_url'][i] = df_lyrics2['lyrics_url'][i]['href']\n",
        "\n",
        "import re\n",
        "\n",
        "for i in range(len(df_lyrics2['lyrics'])):\n",
        "  if len(re.findall('\\t', df_lyrics2['lyrics'][i])) > 0:\n",
        "    df_lyrics2['lyrics'][i] = df_lyrics2['lyrics'][i].replace('\\n', ' ')\n",
        "\n",
        "df_lyrics2.to_csv('df_lyrics2.csv', encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "Mqx0ROIZCdQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 지니\n",
        "\n",
        "* 연도별 곡목록 가져오기"
      ],
      "metadata": {
        "id": "NYTKp6RjA7ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from tqdm import tqdm \n",
        "first_year = 1970\n",
        "last_year = 2023\n",
        "\n",
        "titles, artists, objectIds, years = [], [], [], []\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'\n",
        "    ,'Referer': 'https://www.genie.co.kr/chart/musicHistory?year=1970&category=0&pg=1'\n",
        "}\n",
        "\n",
        "for i in tqdm(range(first_year, last_year)):\n",
        "  \n",
        "  # page1, 2\n",
        "  for page_num in range(1, 3):\n",
        "\n",
        "    url = 'https://www.genie.co.kr/chart/musicHistory?year={0}&category=0&pg={1}'.format(i, page_num)\n",
        "    resp = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(resp.content, 'lxml')\n",
        "\n",
        "    title_tags = soup.select('td.info a.title')\n",
        "    artist_tags = soup.select('td.info a.artist')\n",
        "    objectId_tags = soup.select('td.link a')\n",
        "\n",
        "    print(url, ' 진행중...')\n",
        "\n",
        "    if len(title_tags) == 0:\n",
        "      continue\n",
        "\n",
        "    # print(len(titles), len(artists), len(objectIds))\n",
        "    for j in range(len(title_tags)):\n",
        "      titles.append(title_tags[j].text.replace('\\n', '').replace('TITLE', '').strip())\n",
        "      artists.append(artist_tags[j].text)\n",
        "      objectIds.append(str(objectId_tags[j]['onclick']).replace(\"fnViewSongInfo('\", \"\").replace(\"');return false;\", \"\"))\n",
        "      years.append(i)\n",
        "\n",
        "    print(url, ' 완료...')\n"
      ],
      "metadata": {
        "id": "XpbAipIaBD4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 곡목록 가사 가져오기"
      ],
      "metadata": {
        "id": "6diOVi9KCyF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "lyrics = []\n",
        "\n",
        "cookies = {\n",
        "    'Cookie': 'JSESSIONID=BF2F040D4696F2E511D38677370E96D9; hazelcast.sessionId=HZ7338C44115814C6DBEB51B214A22839E; _BS_GUUID=jgzjHY0WyrQpiJHxPUouUFiAbuJS1X8leHEWFFSe; genie-player-transok=Y; MusicPlayerCookie=128%3B0%3B0%3B1%3B0.5%3BY%3BY%3BN%3B3%3B1%3BAAC; bomb_banner=18987; _TRK_CR=https%3A%2F%2Fwww.google.com%2F; IS%5FKT%5FPRODUCT%5FFIRST=false; _TRK_UID=f8ee3a3f88692eb619438c8c05cfbcfa:6:0.18005677662037037:1667371129754; _TRK_SID=fc10039f21353f23fd996eacdd92cb3e; _TRK_CQ=%3Fkeywd=6bVET930%26source=adwords%26gclid=CjwKCAjwh4ObBhAzEiwAHzZYU8ohZMS3Nn1z2GvrrnLpNZtpMTHVmYiDscQyGfvGxC-lejYMRpid2xoCEKoQAvD_BwE; GENIE%5FUXM=YfzbtN%2BCC22A3COuoRu1ig%3D%3D; GENIE%5FUXD=iPI7jR85CSz%2BsgqmieUdqA%3D%3D; GENIE%5FUXS=SQf1BDscDXVNDfQ6O56fkA%3D%3D; GENIE%5FUXA=0h3IgHa3bnQzkkjOQ87%2F9A%3D%3D; GENIE%5FUXTK=HKOViWbDObZhGLenOVRYIm%2F9ou2X3ePip6GbjD%2F7ds0%3D; GENIE%5FUCF=a9Z5QiaQm9uspPHH3XAYmw%3D%3D; GENIE%5FUCR=SQf1BDscDXVNDfQ6O56fkA%3D%3D; GENIE%5FUXN=BREgSwiK3NUYBfiHtY1JKA%3D%3D; GENIE%5FUST=cdgM08h0S%2F0DAJAGhMbn5A%3D%3D; GENIE%5FUSD=cdgM08h0S%2F0DAJAGhMbn5A%3D%3D; GENIE%5FUXAD=t7zxVcV63%2FI7CuBp89Ov6Q%3D%3D; GENIE%5FP%5FYN=Y; GENIE%5FP%5FNUM=0; GENIE%5F4000%5FFLAG=false; _TRK_RK=324596111; genie-recent=%5B%7B%22word%22%3A%22%uBC15%uC7AC%uBC94%20%uBAB8%uB9E4%22%2C%22date%22%3A%2211.02%22%7D%2C%7B%22word%22%3A%22%uBC15%uC7AC%uBC94%20%uBAB8%uB9E4%22%2C%22date%22%3A%2211.02%22%7D%5D; parentUrl=/detail/songInfo%3Fxgnm%3D84091479; _TRK_EX=30; REMOVE_ALERT_COOKIE_FLAG=324596111%7C%7C20221102163235'\n",
        "}\n",
        "\n",
        "for i in tqdm(range(len(objectIds))):\n",
        "  url = 'https://www.genie.co.kr/detail/songInfo?xgnm={0}'.format(objectIds[i])\n",
        "\n",
        "  resp = requests.get(url, headers=headers, cookies=cookies)\n",
        "  soup = BeautifulSoup(resp.content, 'lxml')\n",
        "  lyrics_tags = soup.select('pre#pLyrics p')\n",
        "\n",
        "  for lyric in lyrics_tags:\n",
        "    lyrics.append(lyric.text.replace('\\n', ' '))\n",
        "\n",
        "  if len(lyrics_tags) == 0:\n",
        "    lyrics.append('NULL')\n",
        "\n",
        "dict_lyrics = {\n",
        "    'title': titles,\n",
        "    'artist': artists,\n",
        "    'year': years,\n",
        "    'lyrics': lyrics\n",
        "}\n"
      ],
      "metadata": {
        "id": "WaT6UB-wC3aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터 정제 및 출력"
      ],
      "metadata": {
        "id": "i-ngqRN1C36N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(titles), len(artists), len(years), len(lyrics))\n",
        "\n",
        "\n",
        "df_lyrics = pd.DataFrame(dict_lyrics)\n",
        "# df_lyrics\n",
        "# df_lyrics.to_csv('genie_lyrics.csv', encoding='utf-8-sig')\n",
        "\n",
        "lyrics2 = lyrics\n",
        "\n",
        "for i in range(len(lyrics2)):\n",
        "  lyrics2[i] = lyrics2[i].replace('\\r', ' ')\n",
        "\n",
        "dict_lyrics = {\n",
        "    'title': titles,\n",
        "    'artist': artists,\n",
        "    'year': years,\n",
        "    'lyrics': lyrics2\n",
        "}\n",
        "\n",
        "df_lyrics = pd.DataFrame(dict_lyrics)\n",
        "# df_lyrics\n",
        "df_lyrics.to_csv('genie_lyrics.csv', encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "zXi6mDOfDALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 각 웹사이트별 출력 자료 반복없이 합치기\n",
        "\n",
        "* 함수 생성"
      ],
      "metadata": {
        "id": "21Wo9MbmErIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가수, 제목, 혹은 가요에 있을 수 있는 빈칸 정리\n",
        "def remove_space(df):\n",
        "  for i in range(len(df)):\n",
        "    df['names'][i] = df['names'][i].strip().replace(' ','')\n",
        "    df['singer'][i] = df['singer'][i].strip()\n",
        "    df['lyrics'][i] = df['lyrics'][i].replace('\\n', ' ')"
      ],
      "metadata": {
        "id": "ZuuY6RH6E6u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 안 겹치는 노래들의 index 찾기\n",
        "def find_idx(df1, df2):\n",
        "  idx_list = []\n",
        "  for i in range(len(df2['names'])):\n",
        "    if df2['names'][i] not in list(df1['names']):\n",
        "      idx_list.append(i)\n",
        "  return idx_list"
      ],
      "metadata": {
        "id": "izBWAe4dFou4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3개의 data 합치기\n",
        "def combine(df1, df2, df3):\n",
        "  new_year = list(df1['year'])\n",
        "  new_singer = list(df1['singer'])\n",
        "  new_names = list(df1['names'])\n",
        "  new_lyrics = list(df1['lyrics'])\n",
        "  for num in find_idx(df1, df2):\n",
        "    new_year.append(df2['year'][num])\n",
        "    new_singer.append(df2['singer'][num])\n",
        "    new_names.append(df2['names'][num])\n",
        "    new_lyrics.append(df2['lyrics'][num])\n",
        "  for num in find_idx(df1, df3):\n",
        "    new_year.append(df3['year'][num])\n",
        "    new_singer.append(df3['singer'][num])\n",
        "    new_names.append(df3['names'][num])\n",
        "    new_lyrics.append(df3['lyrics'][num])\n",
        "  return new_year, new_singer, new_names, new_lyrics"
      ],
      "metadata": {
        "id": "lR1lIpNFFq14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CSV 파일 불러와서, 하나의 파일로 합쳐주기"
      ],
      "metadata": {
        "id": "i_4ncyxTFYQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3개의 csv 불러오기\n",
        "df_melon = pd.read_csv('melon_lyrics.csv', encoding = 'cp949')\n",
        "df_bugs = pd.read_csv('bugs_lyrics.csv')\n",
        "df_genie = pd.read_csv('bugs_lyrics.csv')"
      ],
      "metadata": {
        "id": "XRnVrDnQFdxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈 칸 제거\n",
        "remove_space(df_melon)\n",
        "remove_space(df_bugs)\n",
        "remove_space(df_genie)"
      ],
      "metadata": {
        "id": "3TIrDZt1FfO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나의 csv로 만들어주기\n",
        "year, singer, names, lyrics = combine(df_melon, df_bugs, df_genie)\n",
        "\n",
        "dict_songs ={'year':year,\n",
        "             'singer':singer,\n",
        "             'names':names,\n",
        "             'lyrics':lyrics}\n",
        "\n",
        "df_final = pd.DataFrame(dict_songs)\n",
        "df_final.to_csv('top_songs_final.csv', encoding = 'utf-8')\n",
        "df_final"
      ],
      "metadata": {
        "id": "pUPUtB4jGoCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "TbCaYdGnBGAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 구글 번역\n"
      ],
      "metadata": {
        "id": "dOVGpExSBIaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0 \n",
        "\n",
        "from googletrans import Translator \n",
        "\n",
        "# test\n",
        "translator = Translator()           \n",
        "translations = translator.translate(['The quick brown fox', '뛰다 over', 'the lazy dog'], dest='ko')\n",
        "for translation in translations:\n",
        "  print(translation.origin, ' -> ', translation.text)"
      ],
      "metadata": {
        "id": "sqeIDGraBNA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "translator = Translator() \n",
        "# 파일 읽기\n",
        "df_lyrics = pd.read_excel('song_lyrics.xlsx', index_col=None)\n",
        "\n",
        "for i in range(len(df_lyrics)): # for lyrics in df_lyrics\n",
        "  print(i, df_lyrics['lyrics'].loc[i])\n",
        "  # 가사에서 영어만 추출\n",
        "  lyrics_list = re.findall('[A-Za-z]+', df_lyrics['lyrics'].loc[i])\n",
        "\n",
        "  if len(lyrics_list) > 0:\n",
        "    # 번역\n",
        "    trnaslated_list = translator.translate(lyrics_list, dest='ko')\n",
        "    lyrics_without_english = re.sub('[A-Za-z]+', '', df_lyrics['lyrics'].loc[i])\n",
        "    # 다시 이어붙이기\n",
        "    df_lyrics['lyrics'].loc[i] = \"{0} {1}\".format(lyrics_without_english, \" \".join([translation.text for translation in trnaslated_list])) \n",
        "    print(i, df_lyrics['lyrics'].loc[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "9L6U9WitjFpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lyrics.to_excel('song_lyrics_translated.xlsx')"
      ],
      "metadata": {
        "id": "IZBP_iJO5knk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨링\n",
        "\n",
        "라벨링한 것을 코드로 읽을 수 있는 형태로 변환.\n",
        "\n",
        "처리할 수 있는 방법 2가지\n",
        "\n",
        "* LabelEncoder 사용(Python library)\n",
        "* Dict(or list) 로 바꿔서 있을 경우 해당 컬럼에 1(True)로 처리. ( ✅ )"
      ],
      "metadata": {
        "id": "oZCUCn41Bvsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoder 사용\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "zS_p8bG5WerH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# dict or list 사용\n",
        "# 희망, 돈, 사랑, 효, 우정, 남녀평등, 욕설, 자연, 후회, 추억, SF, 음식, 자신감, 즐거움\n",
        "categories = ['희망', '돈', '사랑', '효', '우정', '남녀평등', '욕설', '자연', '후회', '추억', 'SF', '음식', '자신감', '즐거움']\n",
        "\n",
        "df_lyrics = pd.read_excel('song_lyrics_translated.xlsx', index_col=None)\n",
        "\n",
        "# add category columns to dataframe\n",
        "# df_lyrics.loc[:, ['hope', 'money', 'love']] = [0]*len(df_lyrics)\n",
        "\n",
        "df_lyrics = df_lyrics.assign(hope=[0]*len(df_lyrics)\n",
        "                            , money=[0]*len(df_lyrics)\n",
        "                            , love=[0]*len(df_lyrics)\n",
        "                            , filial=[0]*len(df_lyrics)\n",
        "                            , friend=[0]*len(df_lyrics)\n",
        "                            , equal=[0]*len(df_lyrics)\n",
        "                            , swear=[0]*len(df_lyrics)\n",
        "                            , nature=[0]*len(df_lyrics)\n",
        "                            , regret=[0]*len(df_lyrics)\n",
        "                            , memory=[0]*len(df_lyrics)\n",
        "                            , sf=[0]*len(df_lyrics)\n",
        "                            , food=[0]*len(df_lyrics)\n",
        "                            , confi=[0]*len(df_lyrics)\n",
        "                            , enjoy=[0]*len(df_lyrics))\n",
        "df_lyrics"
      ],
      "metadata": {
        "id": "AfiN2kdIWgWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1A3Np8iKee0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 희망, 돈, 사랑, 효, 우정, 남녀평등, 욕설, 자연, 후회, 추억, SF, 음식, 자신감, 즐거움\n",
        "from tqdm import tqdm\n",
        "\n",
        "for index in tqdm(range(len(df_lyrics))):\n",
        "  if \"희망\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['hope'].loc[index] = 1\n",
        "  if \"돈\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['money'].loc[index] = 1\n",
        "  if \"사랑\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['love'].loc[index] = 1\n",
        "  if \"효\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['filial'].loc[index] = 1\n",
        "  if \"우정\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['friend'].loc[index] = 1\n",
        "  if \"남녀평등\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['equal'].loc[index] = 1\n",
        "  if \"욕설\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['swear'].loc[index] = 1\n",
        "  if \"자연\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['nature'].loc[index] = 1\n",
        "  if \"후회\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['regret'].loc[index] = 1\n",
        "  if \"추억\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['memory'].loc[index] = 1\n",
        "  if \"SF\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['sf'].loc[index] = 1\n",
        "  if \"sf\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['sf'].loc[index] = 1\n",
        "  if \"음식\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['food'].loc[index] = 1\n",
        "  if \"자신감\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['confi'].loc[index] = 1\n",
        "  if \"즐거움\" in df_lyrics['categories'].loc[index].replace(' ', '').split(','): \n",
        "    df_lyrics['enjoy'].loc[index] = 1"
      ],
      "metadata": {
        "id": "7PW160dQWh6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd2ce25-9873-4fd3-954a-81423206f110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6200 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n",
            "100%|██████████| 6200/6200 [00:05<00:00, 1239.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lyrics.to_excel('song_lyrics_labeled.xlsx')"
      ],
      "metadata": {
        "id": "VfVOYvySWkPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_categories_sum.sort_values([\"hope\"], ascending=[False]).head(10)"
      ],
      "metadata": {
        "id": "eL2IV8nge3l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 빈도수 확인\n",
        "\n",
        "불용어를 빈도 수 많은 토큰을 우선으로 살펴본 후 선정하기 위해."
      ],
      "metadata": {
        "id": "mX9Vk32FCqex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링한 모든 노래에 대하여, 특정 단어의 출현 빈도수 \n",
        "def find_frequency(df):\n",
        "\n",
        "  lst = ['Noun', 'Adjective', 'Adverb', 'Verb']\n",
        "  final = []\n",
        "  \n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    test_txt = re.sub(\"[^가-힣\\\\s]\", \"\", df['lyrics'].iloc[i])\n",
        "    test_review = okt.pos(test_txt, stem=True)\n",
        "    for j in range(len(test_review)):\n",
        "      if test_review[j][1] in lst:\n",
        "        final.append(test_review[j][0])\n",
        "\n",
        "  num_counts = Counter(final)\n",
        "  sorted_counts = sorted(num_counts.items(), key=lambda x:x[1])\n",
        "\n",
        "  return sorted_counts"
      ],
      "metadata": {
        "id": "QphDyf5v2eJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count는 각 단어별로 개수\n",
        "count = find_frequency(songs)"
      ],
      "metadata": {
        "id": "KqwBK-ou2wZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어마다 문서 출현 빈도수 측정"
      ],
      "metadata": {
        "id": "LIgQ_HnE3Amq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_list = []\n",
        "# 노래 가사에 나온 단어들만 추출해낸 리스트 생성\n",
        "for i in range(len(count)):\n",
        "  dic_list.append(count[i][0])\n",
        "\n",
        "dic_list"
      ],
      "metadata": {
        "id": "ea5EkYoX3Hrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어를 key 로 설정하고 value 는 모두 0으로 지정\n",
        "dictionary = dict.fromkeys(dic_list, 0)\n",
        "\n",
        "dictionary"
      ],
      "metadata": {
        "id": "hW1QqoMp3NBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노래별 가사를 토큰화시킨 리스트를 이중 리스트로 작성\n",
        "def clean_lyrics(df):\n",
        "\n",
        "  lst = ['Noun', 'Adjective', 'Adverb', 'Verb']\n",
        "  final = []\n",
        "  \n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    result = []\n",
        "    test_txt = re.sub(\"[^가-힣\\\\s]\", \"\", df['lyrics'].iloc[i])\n",
        "    test_review = okt.pos(test_txt, stem=True)\n",
        "    for j in range(len(test_review)):\n",
        "      if test_review[j][1] in lst:\n",
        "        result.append(test_review[j][0])\n",
        "    final.append(result)\n",
        "  \n",
        "  return final"
      ],
      "metadata": {
        "id": "cJy6fMNY3Rq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가사 전처리 및 토큰화\n",
        "lyrics_list = clean_lyrics(songs)"
      ],
      "metadata": {
        "id": "-qs1dUUa3o_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 빈도수 측정\n",
        "def doc_frequency(lyrics, dictionary):\n",
        "\n",
        "  for lyric in tqdm(lyrics):\n",
        "    num_count = Counter(lyric)\n",
        "    for word in dic_list:\n",
        "      if num_count[word] > 0:\n",
        "        dictionary[word] += 1\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "45x-tAxo3tw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_frequency(lyrics_list, dictionary)"
      ],
      "metadata": {
        "id": "Xb6UyyzK4WEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 리턴된 데이터에서 df 확인 후, df 상위 2% 데이터를 직접 검토하여 불용어 사전 작성"
      ],
      "metadata": {
        "id": "CZRxtlGh4jsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 16000개 가량의 단어중에 상위 2% 문서 출현 빈도 단어들 중, 불용어로 지정할 단어들만 모은 리스트를 생성\n",
        "stop_list = ['잠', '바', '워', '몸', '진짜', '언젠가', '죽다', '힘', '뭐라다', '절대', '그저', '애', '하얗다', '돌다', '누가', '크다', '늘', '다시다', '뜨겁다', '아침', '가지', '숨', '상처', '뛰다', '순', '듣다', '머리', '단지', '이상', '꼭', '어리다', '기다', '따르다', '채', '향', '떨어지다', '듯', '예', '뜨다', '아직도', '갖다', '전', '나오다']\n",
        "stop_list2 = ['받다', '때문', '지다', '서다', '이름', '많이', '수가', '조금', '아래', '두', '좀', '의', '무엇', '한번']\n",
        "stop_list3 = ['줄', '처럼', '누구', '뿐', '만들다', '게', '순간', '어디', '미치다', '다른', '길', '뭐', '음', '안녕', '저', '건', '하루', '그래서', '여기', '이렇다', '손', '그렇게','우', '자다', '많다', '노래', '뒤', '응', '곳', '말다', '거', '정말', '대다', '이다', '웃다', '못', '하늘', '에스', '어떻다', '없이', '해주다', '곁', '나다', '티', '모든', '이렇게', '들다']\n",
        "stop_list4 = ['하다', '있다', '없다', '말', '되다', '그렇다', '생각', '걸', '그', '것', '오', '이', '다', '속','해',\n",
        " '돼다', '알', '젠', '나르다', '주다', '아기', '중', '사람', '오다','가다','않다']"
      ],
      "metadata": {
        "id": "eF8xPALc4iWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list.extend(stop_list2)\n",
        "stop_list.extend(stop_list3)\n",
        "stop_list.extend(stop_list4)\n",
        "print(stop_list)"
      ],
      "metadata": {
        "id": "5Zb9MDzL5eel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어 사전을 이용하여 최종 전처리 작업\n",
        "def preprocessing(review): \n",
        "    okt = Okt()\n",
        "    \n",
        "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
        "    review_text = re.sub(\"[^가-힣\\\\s]\", \"\", review)\n",
        "    \n",
        "    # 2. okt 객체를 활용해서 형태소 토큰화 + 품사 태깅\n",
        "    word_review = okt.pos(review_text, stem=True)\n",
        "    \n",
        "    # 노이즈 & 불용어 제거\n",
        "    word_review = [(token, pos) for token, pos in word_review if not token in stop_list]\n",
        "    \n",
        "    # 명사, 동사, 형용사 추출\n",
        "    word_review = [token for token, pos in word_review if pos in ['Noun', 'Verb', 'Adjective', 'Adverb']]\n",
        "\n",
        "    return word_review"
      ],
      "metadata": {
        "id": "hVUM1WSV5l5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화하여, X_train과 X_test tfidf 형태로 생성"
      ],
      "metadata": {
        "id": "hSRxJ5Vn501w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=preprocessing, max_features=3000, min_df=5, max_df=0.5) \n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "X_train_tfidf.toarray()"
      ],
      "metadata": {
        "id": "1Xg58do66FBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GridSearchCV"
      ],
      "metadata": {
        "id": "2nGLRNEDZL-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델링"
      ],
      "metadata": {
        "id": "_B-RnPJ_6Pa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 생성"
      ],
      "metadata": {
        "id": "rUTPh2CA6VtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 분류모델을 담을 리스트 생성 \n",
        "classifiers = []\n",
        "\n",
        "# random_state 설정\n",
        "random_state = 2\n",
        "\n",
        "# 데이터프레임 생성을 위하여 알고리즘 제목 저장\n",
        "clf_names = ['Decision Tree','Random Forest', 'AdaBoost', 'Extra Trees', 'Gradient Boost',\n",
        "             'MLP', 'KNN', 'Support Vector', 'Logistic Regression', 'Linear Discrimination']\n",
        "\n",
        "# 트리 기반 알고리즘\n",
        "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
        "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
        "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
        "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
        "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
        "classifiers.append(MLPClassifier(random_state=random_state))\n",
        "\n",
        "# 그 외 선형, 신경망 등의 알고리즘\n",
        "classifiers.append(KNeighborsClassifier())\n",
        "classifiers.append(SVC(random_state=random_state))\n",
        "classifiers.append(LogisticRegression(random_state = random_state))\n",
        "classifiers.append(LinearDiscriminantAnalysis())"
      ],
      "metadata": {
        "id": "aLN1kZ7L6SRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블인코딩된 데이터로 교차검증을 수행하여 리스트에 저장\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10)\n",
        "\n",
        "cv_results = []\n",
        "for classifier in classifiers :\n",
        "    cv_results.append(cross_val_score(classifier, X_train_tfidf, y = y_train,\n",
        "                                      scoring = \"accuracy\", cv = kfold, n_jobs=4))"
      ],
      "metadata": {
        "id": "1j1wGdc-6ZxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차검증 점수 리스트 출력\n",
        "cv_results"
      ],
      "metadata": {
        "id": "3VXX9JbP6dua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델들의 교차검증 점수의 평균과 표준편차를 계산하여 리스트에 저장\n",
        "cv_means = []\n",
        "cv_std = []\n",
        "\n",
        "for cv_result in cv_results:\n",
        "    cv_means.append(cv_result.mean())\n",
        "    cv_std.append(cv_result.std())"
      ],
      "metadata": {
        "id": "PJxmF2bP6oQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임으로 변환\n",
        "results_le = pd.DataFrame(cv_results, index=clf_names)\n",
        "results_le"
      ],
      "metadata": {
        "id": "SUK1xD8C6rNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델별 교차검증 점수 평균, 표준편차를 새로운 칼럼으로 추가\n",
        "results_le['mean'] = cv_means\n",
        "results_le['std'] = cv_std"
      ],
      "metadata": {
        "id": "s2XU8Hnu6u3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  모델별 교차검증 점수 평균 시각화\n",
        "g = sns.barplot(\"mean\",results_le.index ,data = results_le, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
        "g.set_xlabel(\"Mean Accuracy\")\n",
        "g = g.set_title(\"Cross validation scores\")"
      ],
      "metadata": {
        "id": "Ui6B30Zx6vVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- kfold 를 10 으로 설정시에 다양한 모델을 돌리기에 다소 무리가 있었으므로, 데이터가 편향도를 확인 후, 그렇지 않을 시에 kfold를 낮추기로 결정"
      ],
      "metadata": {
        "id": "VVxEZALQ62NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 평균으로부터 2 표준편차 이상 벗어난 데이터 포인트가 데이터의 10%를 넘지 않을 시, kfold update\n",
        "def find_outliers(df):\n",
        "  lst = []\n",
        "  rows = df.shape[0]-1\n",
        "  cols = df.shape[1]-2\n",
        "  for i in range(0,rows):\n",
        "    for j in range(0,cols):\n",
        "      if df.iloc[i][j] > df.iloc[i]['mean'] + 2*df.iloc[i]['std'] or df.iloc[i][j] < df.iloc[i]['mean'] - 2*df.iloc[i]['std']:\n",
        "        lst.append((i,j))\n",
        "  if len(lst) < 0.1 * (rows * cols):\n",
        "    print('Update Kfold')\n",
        "  else:\n",
        "    print('Do not change Kfold')"
      ],
      "metadata": {
        "id": "bebCnciH61m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_outliers(results_le)"
      ],
      "metadata": {
        "id": "wkxu0Bq-6w67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=3)"
      ],
      "metadata": {
        "id": "5eoNzdqE7Pwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼 파라미터 튜닝"
      ],
      "metadata": {
        "id": "P8QcnRgy7VYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "fH8kx6R37rje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest 객체 생성\n",
        "RFC = RandomForestClassifier(random_state=7)\n",
        "\n",
        "\n",
        "# param_grid 설정\n",
        "rf_param_grid = {\"max_depth\": [None],\n",
        "              \"min_samples_split\": [2, 3, 10],\n",
        "              \"min_samples_leaf\": [1, 3, 10],\n",
        "              \"bootstrap\": [False],\n",
        "              \"n_estimators\" :[100,300],\n",
        "              \"criterion\": [\"gini\"]}\n",
        "\n",
        "# Grid Search 객체 생성\n",
        "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsRFC.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "XPpCLVVf7gDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "RFC_best = gsRFC.best_estimator_\n",
        "RFC_params = gsRFC.best_params_\n",
        "print(RFC_best)\n",
        "print(RFC_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsRFC.best_score_)"
      ],
      "metadata": {
        "id": "b792RcTM7iMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN (K-Nearest Neighbors)"
      ],
      "metadata": {
        "id": "HkG90sK17i7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN 객체 생성\n",
        "KNN = KNeighborsClassifier()\n",
        "\n",
        "# param_grid 설정\n",
        "knn_param_grid = {\"n_neighbors\" : [1, 2, 3, 4, 5], \n",
        "                  \"algorithm\": ['auto'],\n",
        "                  \"weights\" : ['uniform', 'distance'],\n",
        "                  \"leaf_size\" : [1, 2, 3, 4, 5, 10]}\n",
        "\n",
        "# Grid Search 객체 생성\n",
        "gsKNN = GridSearchCV(KNN, param_grid = knn_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsKNN.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "Ki2mzvr97oEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "KNN_best = gsKNN.best_estimator_\n",
        "KNN_params = gsKNN.best_params_\n",
        "print(KNN_best)\n",
        "print(KNN_params)\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsKNN.best_score_)"
      ],
      "metadata": {
        "id": "jti5WFFC70gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost"
      ],
      "metadata": {
        "id": "hwfJXJAq8HTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost 객체 생성\n",
        "ADA = AdaBoostClassifier(random_state=7)\n",
        "\n",
        "# param_grid 설정\n",
        "ada_param_grid = {\"n_estimators\" :[1,20,30],\n",
        "                  \"algorithm\" : [\"SAMME\",\"SAMME.R\"],                  \n",
        "              \"learning_rate\":  [0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
        "\n",
        "# Grid Search 객체 생성\n",
        "gsADA = GridSearchCV(ADA, param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsADA.fit(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "sA6DuY748PeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "ADA_best = gsADA.best_estimator_\n",
        "ADA_params = gsADA.best_params_\n",
        "print(ADA_best)\n",
        "print(ADA_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsADA.best_score_)\n"
      ],
      "metadata": {
        "id": "uRCYjK398P0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boost"
      ],
      "metadata": {
        "id": "fX6upYry8P7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient boost 객체 생성\n",
        "GBC = GradientBoostingClassifier(random_state=7)\n",
        "\n",
        "# param_grid 설정\n",
        "gb_param_grid = {'loss' : [\"deviance\"],\n",
        "              'n_estimators' : [100,200,300],\n",
        "              'learning_rate': [0.1, 0.05, 0.01],\n",
        "              'max_depth': [4, 8],\n",
        "              'min_samples_leaf': [100,150],\n",
        "              }\n",
        "# Grid Search 객체 생성\n",
        "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsGBC.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "s93FPU2Q8Wzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "GBC_best = gsGBC.best_estimator_\n",
        "GBC_params = gsGBC.best_params_\n",
        "print(GBC_best)\n",
        "print(GBC_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsGBC.best_score_)"
      ],
      "metadata": {
        "id": "n7ntmll-8W81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Trees"
      ],
      "metadata": {
        "id": "NCIfnh218XB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtraTrees 객체 생성\n",
        "Ext = ExtraTreesClassifier(random_state=7)\n",
        "\n",
        "\n",
        "# param_grid 설정\n",
        "ex_param_grid = {\"max_depth\": [None],\n",
        "              \"min_samples_split\": [2, 3, 10],\n",
        "              \"min_samples_leaf\": [1, 3, 10],\n",
        "              \"bootstrap\": [False],\n",
        "              \"n_estimators\" :[100,300],\n",
        "              \"criterion\": [\"gini\"]}\n",
        "\n",
        "# Grad Search 객체 생성\n",
        "gsExt = GridSearchCV(Ext,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsExt.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "iwFc6iNz8dPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "Ext_best = gsExt.best_estimator_\n",
        "Ext_params = gsExt.best_params_\n",
        "print(Ext_best)\n",
        "print(Ext_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsExt.best_score_)"
      ],
      "metadata": {
        "id": "lvgve7_m8dcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "SuH6Eyuy8d0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree 객체 생성\n",
        "DTC = DecisionTreeClassifier(random_state=7, )\n",
        "\n",
        "# param_grid 설정\n",
        "dtc_param_grid = {\"criterion\" : [\"gini\", \"entropy\"],\n",
        "              \"min_samples_split\" :   [2, 4, 6, 10],\n",
        "              \"min_samples_leaf\" : [1,3, 5, 10]}\n",
        "\n",
        "# Grad Search 객체 생성\n",
        "gsDTC = GridSearchCV(DTC,param_grid = dtc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsDTC.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "K7qRFOvc8jg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "DTC_best = gsDTC.best_estimator_\n",
        "DTC_params = gsDTC.best_params_\n",
        "print(DTC_best)\n",
        "print(DTC_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsDTC.best_score_)"
      ],
      "metadata": {
        "id": "75zDLJSN8jxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector"
      ],
      "metadata": {
        "id": "VSd4VlS58npc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Support Vector 객체 생성\n",
        "SVM = SVC(random_state=7, probability=True)\n",
        "\n",
        "# param_grid 설정\n",
        "svc_param_grid = {'kernel': ['rbf'], \n",
        "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
        "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
        "\n",
        "# Gradient boost 객체 생성\n",
        "gsSVM = GridSearchCV(SVM,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsSVM.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "RqhPigsE8q6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "SVM_best = gsSVM.best_estimator_\n",
        "SVM_params = gsSVM.best_params_\n",
        "print(SVM_best)\n",
        "print(SVM_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsSVM.best_score_)"
      ],
      "metadata": {
        "id": "kdhS-pap8tE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP(Multi-layer Perceptron)"
      ],
      "metadata": {
        "id": "nDCZhzGi8vDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Discrimination Analysis 객체 생성\n",
        "MLP = MLPClassifier()\n",
        "\n",
        "# param_grid 설정\n",
        "mlp_param_grid = {'hidden_layer_sizes': [100, 150, 200, 250], \n",
        "                #   'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "                #   'solver': ['lbfgs','sgd','adam'],\n",
        "                  'alpha': [ 0.0001, 0.001, 0.01],\n",
        "                #   'learning_rate': ['const  ant','invscaling','adaptive']\n",
        "                }\n",
        "\n",
        "# Gradient boost 객체 생성\n",
        "gsMLP = GridSearchCV(MLP,param_grid = mlp_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsMLP.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "m3cOkL2U8xiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "MLP_best = gsMLP.best_estimator_\n",
        "MLP_params = gsMLP.best_params_\n",
        "print(MLP_best)\n",
        "print(MLP_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsMLP.best_score_)\n"
      ],
      "metadata": {
        "id": "xc3kdEf-8zSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "k3wGw8Rr81D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression 객체 생성\n",
        "LGS = LogisticRegression(random_state=7)\n",
        "\n",
        "# param_grid 설정\n",
        "lgs_param_grid = {'penalty': ['l1','l2'], \n",
        "                  'C': np.logspace(0,4,10)}\n",
        "\n",
        "# Gradient boost 객체 생성\n",
        "gsLGS = GridSearchCV(LGS,param_grid = lgs_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
        "\n",
        "# Grid Search 실행\n",
        "gsLGS.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "fnKFK1nz83F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 파라미터 조합 출력\n",
        "LGS_best = gsLGS.best_estimator_\n",
        "LGS_params = gsLGS.best_params_\n",
        "print(LGS_best)\n",
        "print(LGS_params)\n",
        "\n",
        "# 최고 점수(accuracy) 출력\n",
        "print(gsLGS.best_score_)"
      ],
      "metadata": {
        "id": "cNGBaXLt89HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting"
      ],
      "metadata": {
        "id": "mEXaknbV8-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#튜닝한 파라미터로 하드보팅한 후 점수를 출력해보자.\n",
        "grid_hard = VotingClassifier(estimators = [('Decision Tree', DTC_best), \n",
        "                                           ('Random Forest', RFC_best),\n",
        "                                           ('ExtraTrees', Ext_best),\n",
        "                                           ('SVC', SVM_best),\n",
        "                                           ('AdaBoost', ADA_best),\n",
        "                                           ('GradientBoosting', GBC_best),\n",
        "                                           ('KNN', KNN_best),\n",
        "                                           ('MLP', MLP_best),\n",
        "                                           ('Logistic Regression', LGS_best)], voting = 'hard')\n",
        "\n",
        "grid_hard_cv = model_selection.cross_validate(grid_hard, X_train_tfidf, y_train, cv=10)\n",
        "\n",
        "print(\"Hard voting on test set score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean() * 100))"
      ],
      "metadata": {
        "id": "t9CRon039CdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#튜닝한 파라미터로 하드보팅\n",
        "grid_soft = VotingClassifier(estimators = [('Decision Tree', DTC_best), \n",
        "                                           ('Random Forest', RFC_best),\n",
        "                                           ('ExtraTrees', Ext_best),\n",
        "                                           ('SVC', SVM_best),\n",
        "                                           ('AdaBoost', ADA_best),\n",
        "                                           ('GradientBoosting', GBC_best),\n",
        "                                           ('KNN', KNN_best),\n",
        "                                           ('MLP', MLP_best),\n",
        "                                           ('Logistic Regression', LGS_best)], voting = 'soft')\n",
        "\n",
        "grid_soft_cv = model_selection.cross_validate(grid_soft, X_train_tfidf, y_train, cv=10)\n",
        "\n",
        "print(\"Soft voting on test set score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean() * 100))"
      ],
      "metadata": {
        "id": "fCyAvhbj9FAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Curve"
      ],
      "metadata": {
        "id": "yLGU8xfa9J72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
        "    # 그래프 설정\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "\n",
        "    # learning curve\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    \n",
        "    # 학습 데이터에 대한 점수의 평균\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    # 학습 데이터에 대한 점수의 표준편차\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    # 검증 데이터에 대한 점수의 평균\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    # 검증 데이터에 대한 점수의 표준편차\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    \n",
        "    # 그래프에 눈금 표시\n",
        "    plt.grid()\n",
        "\n",
        "    # fill_between() = 그래프의 두 수평 방향의 곡선 사이를 색상으로 채워서 강조\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    \n",
        "    # 데이터를 연결하는 선을 plot\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    # 범례 표시\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "metadata": {
        "id": "sAydiZk49MC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = plot_learning_curve(DTC_best,\"Decision Tree learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(RFC_best,\"Random Forest learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(Ext_best,\"ExtraTrees learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(SVM_best,\"SVC learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(ADA_best,\"AdaBoost learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(GBC_best,\"GradientBoosting learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(KNN_best,\"KNN learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(MLP_best,\"MLP learning curves\",X_train_tfidf,y_train,cv=kfold)\n",
        "g = plot_learning_curve(LGS_best,\"Logistic Regression learning curves\",X_train_tfidf,y_train,cv=kfold)"
      ],
      "metadata": {
        "id": "IzqIs4j39O37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Labeling"
      ],
      "metadata": {
        "id": "RDDh3TyfAdk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- multi labeling 작업을 위한 array 생성"
      ],
      "metadata": {
        "id": "RZuDqwEAAf3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_df(df):\n",
        "  full = []\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sem = []\n",
        "    for j in range(8,22):\n",
        "      sem.append(songs.iloc[i][j])\n",
        "    full.append(sem)\n",
        "  return full"
      ],
      "metadata": {
        "id": "kwsoVwg8AmaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full = combine_df(songs)\n",
        "songs['full'] = full"
      ],
      "metadata": {
        "id": "gK4KFAbXAo8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = songs.lyrics\n",
        "y = songs.full"
      ],
      "metadata": {
        "id": "6wItWaaQArGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(y.tolist())"
      ],
      "metadata": {
        "id": "kiBDVJhVArPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .8)"
      ],
      "metadata": {
        "id": "Dgo73x3FArVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=preprocessing, max_features=3000, min_df=5, max_df=0.5) \n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "X_train_tfidf.toarray()"
      ],
      "metadata": {
        "id": "x9HM9g6XAwD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델별 결과 도출"
      ],
      "metadata": {
        "id": "e269bd9aA0gC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Random Froest "
      ],
      "metadata": {
        "id": "d5YEEN4zA59z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RFC = MultiOutputClassifier(estimator= RandomForestClassifier()).fit(X_train_tfidf, y_train)\n",
        "RFC.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "T-ZyskC9A4te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFC_score = RFC.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "ZPSsUT4hA_Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. KNN (K-Nearest Neighbors)"
      ],
      "metadata": {
        "id": "wSiL1dWaBCLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN = MultiOutputClassifier(estimator= KNeighborsClassifier()).fit(X_train_tfidf, y_train)\n",
        "KNN.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "Hht6nuoUBEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_score = KNN.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "YVrChHWuBGbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Ada boost"
      ],
      "metadata": {
        "id": "uFe6lmnuBJCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ADA = MultiOutputClassifier(estimator= AdaBoostClassifier()).fit(X_train_tfidf, y_train)\n",
        "ADA.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "kkC7-Ot9BMeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADA_score = ADA.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "jNVn5UjABM1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Gradient Boost"
      ],
      "metadata": {
        "id": "zJ6nfkzSBM-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRD = MultiOutputClassifier(estimator= GradientBoostingClassifier()).fit(X_train_tfidf, y_train)\n",
        "GRD.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "CkPovHOXBNGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRD_score = GRD.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "xjJhhUHABNOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Extra Trees"
      ],
      "metadata": {
        "id": "M7AX_u7jBVBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXT = MultiOutputClassifier(estimator= ExtraTreesClassifier()).fit(X_train_tfidf, y_train)\n",
        "EXT.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "7TuhEXDaBXVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXT_score = EXT.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "-33GEwgNBZ9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Decision Tree"
      ],
      "metadata": {
        "id": "_GqP-zqHBaj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DTC = MultiOutputClassifier(estimator= DecisionTreeClassifier()).fit(X_train_tfidf, y_train)\n",
        "DTC.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "yDBSyLKpBar-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTC_score = DTC.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "94YZIDBxBe9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. SVC"
      ],
      "metadata": {
        "id": "_wfzyS3BBgYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = MultiOutputClassifier(estimator= SVC()).fit(X_train_tfidf, y_t)\n",
        "SVM.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "WhsKCoRZBssM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_score = SVM.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "5u2KKN5eBudN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. MLP(Multi-layer Perceptron)"
      ],
      "metadata": {
        "id": "96XAfWcpBv3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MultiOutputClassifier(estimator= MLPClassifier()).fit(X_train_tfidf, y_train)\n",
        "MLP.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "AzUHkL5ZBxyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP_score = MLP.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "S9g2lt-kBzgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Logistic Regression"
      ],
      "metadata": {
        "id": "qVsDo5wRB0v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LGS = MultiOutputClassifier(estimator= LogisticRegression()).fit(X_train_tfidf, y_train)\n",
        "LGS.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "9axuaTuoB3VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGS_score = LGS.score(X_test_tfidf, y_test)"
      ],
      "metadata": {
        "id": "gchdjngzB5GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar graph"
      ],
      "metadata": {
        "id": "PM2rUkzxB9rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_dict = {'RFC':RFC_score,\n",
        "              'KNN':KNN_score,\n",
        "              'ADA':ADA_score,\n",
        "              'GRD':GRD_score,\n",
        "              'EXT':EXT_score,\n",
        "              'DTC':DTC_score,\n",
        "              'SVM':SVM_score,\n",
        "              'MLP':MLP_score,\n",
        "              'LGS':LGS_score}"
      ],
      "metadata": {
        "id": "_ta16V5nB6w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(range(len(multi_dict)), list(multi_dict.values()), align='center')\n",
        "plt.xticks(range(len(multi_dict)), list(multi_dict.keys()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zWpOUN3UCBv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(RFC_score, KNN_score, ADA_score, GRD_score, EXT_score, DTC_score, SVM_score, MLP_score, LGS_score)"
      ],
      "metadata": {
        "id": "uH3KCd9jCEDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}